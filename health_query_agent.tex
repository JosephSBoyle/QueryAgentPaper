\documentclass[11pt]{article}

% custom packages: %

\usepackage[dvipsnames]{xcolor}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{xurl}

\usepackage{standalone}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, fit}

\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%%

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{HealthQueryAgent: A Task-Oriented Dialogue System for Automated Clinical Informatics}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Test Edit From Overleaf Syncs \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}

% load results values
\include{results_development}

\maketitle
\begin{abstract}
% What does this paper add
In this paper we describe \textbf{HealthQueryAgent,} a Task-Oriented Dialogue (TOD) for performing population health management operations. We deploy the system in a familiar chat interface embedded within an existing population health management platform and run experiments to determine it's utility to a diverse userbase of 2,000 analysts and clinicians querying the data of 1,279,215 million active patients. We evaluate the agent's capacity to autonomously handle a range of health informatics tasks.
% We compare a range of frontier LLMs on a benchmark comprised of real-world user (question, query) pairs, and find (TBD). % todo

% Analysing healthcare data can be an arduous and error-prone task when done manually. 
The aim of population health analysis is for a user to generate some knowledge from a database by conducting a database query to access information regarding a set of specified patients. 
% Three examples are: listing patients who meet some criteria; calculating the rate of a condition within a subpopulation; and measuring the performance of their organisation according to some metrics.
Specialist analysts are needed to translate clinical queries into database ones, imposing a steep cost and introducing a considerable delay. We hypothesise a system for automatically performing this translation in an interpretable fashion could drastically reduce both the cost and the latency to performing population health analysis, and enable clinicians to rapidly generate actionable knowledge from clinical data.

% This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style files for ACL conferences.
% The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
% These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\begin{figure}[htbp]
\centering
\includestandalone{figure\_one}
\caption{
HealthQueryAgent parses natural language questions into database queries in an agentic loop, dynamically determining the course of action as it interacts with the data environment.
The data environment comprises a local database of analyst-created medical concepts, and an open-access knowledge-base, UMLS, which is used to help ground the agent faithfully map clinical surface forms (e.g. `ddC') to local concepts, a process which involves multi-step information retrieval.
HealthQueryAgent hands off the \textit{retrieval} of concepts to a sub-agent so that it can focus on \textit{composing} the database query.}
\label{fig:question-to-query}

\end{figure}

\section{Introduction}

% focus has been elsewhere
Healthcare organisations collect vast quantities of healthcare data, however at present it would be difficult to argue that this data is used to it's \textit{full potential}. Whilst there has been a concerted effort from machine learning researchers to automate various extractive medical tasks such as clinical coding and radiology report generation, relatively few studies have investigated how to maximise clinical use of this information: leaving large population health benefits `on the table' so to speak.

% Ensure this is rendered on page 2.

% \begin{figure*}[t]
%   \includegraphics[width=\linewidth]{content/system_diagram3.png}
% 	\caption{System diagram showing a subset of HealthQueryAgent's tools. The agent must enact multi-step plans using a concept database and a medical knowledge base to create an analysis query, which is then passed to the user who will be able to view the retrieved results. To ensure privacy, the agent's tools cannot access the Patient EHR database. The user may interactively refine the query by conversing with the agent, for instance prompting it to modify its results or asking follow up questions.}
% 	\label{fig:system-diagram}
% \end{figure*}


% Formulation
\subsection{Task}
This paper attempts to generate population health queries from real-world user questions.
This is a complex challenge encompassing a wide range of abilities: determining the user's intent (semantic parsing); disambiguating complex and often overloaded medical language; retrieving and selecting the appropriate medical concepts; and finally creating a query.
We make the following three contributions:

\begin{enumerate}

\item We demonstrate that it is possible to automate the mapping from a clinical question to a clinically useful result. Formally, given a clinical user's question $Q$ and a database of patients, medical concepts (codes) and code occurrences (events) $DB$,

  $$ DB = (patients, concepts, events) $$

we construct a system, $M$, which maps unseen user questions to a useful answer $A$:

  $$ M(DB): \mathcal{Q} \rightarrow \mathcal{A} $$

\item We introduce a simple and powerful mechanism for reducing the performance penalty incurred during multi-turn conversations. % enabling agents to maintain coherence over long

\item We demonstrate that providing access to an external KB aids in the interpretation and mapping of specialist queries involving niche medical concepts.
\end{enumerate}

% Does providing an external knowledge base (KB) enhance the agent's ability to generate queries? Prior studies show that LLMs perform worse on rarer medical concepts. Does an external KB benefit queries for rare medical concepts more than common ones?

\subsection{Task-Oriented Dialogue and LLM Agents}
% todo: make more relevant to our work.
Task-Oriented Dialogues (TOD) are ones in which an agent and user collaborate to act out a user's goal.
This is in contrast to open-domain dialogue, which is not limited to the domain of any specific task.
A sub-task of TOD is semantic parsing, the process of creating logical interpretations of an instruction / utterance in the context of the dialogue thus far in order to determine what action to perform next.

Traditional TOD systems typically consist of a pipeline of distinct modules, each with a specific responsibility.
A typical modular architecture might include a component for understanding the user's intent and extracting structured information from it (NLU); a component for tracking the state of the dialogue (DST); a policy module for determining the next action; and a final natural languge generation model for synthesising a response.  

As pointed out by \citet{yi_survey_2025}, the propagation of errors between modules hampers the performance of these systems, and they have largely fallen to the wayside in favour of systems which utilize a single sequence-to-sequence model to perform the entire task.
Examples of the latter include \citet{wen_network-based_2017}, who developed a novel NN architecture and \citet{andreas_task-oriented_2020} were the first to treat TOD as a data flow graph in which the agent iteratively generates the program as the conversation progresses.

\citet{hosseini-asl_simple_2022} created SimpleTOD, an end-to-end architecture utilising only a single pre-trained language model, GPT-2, to perform all aspects of task completion including database calls, with the results concatenated to the context.
In sum, recent progress in TOD systems has been to move away from modular architectures and towards unified ones, epitomised by the SimpleTOD approach, which uses only a finetuned LLM. 


% Two key challenges in TOD maintaining coherence over long multi-turn conversations (\citet{laban_llms_2025}), as well as identifying when f under-specified questions\cite{gan_clarq-llm_2024}
In 2025, the most common form of TOD agents, referred to simply as `agents', are coding ones such as Claude Code, which are capable of performing programming tasks with 50\% success 
been demonstrated to \cite{kwa_measuring_2025}.
%LLM agents (agents) refer to instruction-tuned models which are augmented with tools (functions) which enable them to take actions within an environment e.g. retrieving information from Wikipedia in order to obtain an answer \cite{yao_reac_2023}.
Similar to SimpleTOD executing database commands based on autoregressive modelling of the user's belief state, state-of-the-art agents issue what's known as `tools-calls' through autoregressive language modelling. This is referred to as the `ReACT' paradigm, after the seminal paper from \citet{yao_reac_2023} which popularised the approach. To conclude, LLM agents are a new and powerful approach to goal-oriented dialogue systems, in which the agent can autonomously and iteratively interact with it's environment in order to accomplish the user's goals.


\subsection{Agentic Information Retrieval (IR)}
Agentic IR (AIR) is a new paradigm in which language models iteratively interact with the environment in the `ReAct' loop to retrieve information \citet{zhang_agentic_2025}.
The key of an agentic approach to IR over a traditional one is the ability to adapt their search as information is collected.
%This makes the agentic approach more robust to various linguistic phenomena.

Agentic IR methods are a rapid area of development, with successful systems employing a variety of different tools for agents to IR.
For instance, the current SOTA coding agent, Claude Code, uses only a lexical search function, akin to UNIX's `grep' tool to retrieve files, whilst Windsurf, a popular agentic coding IDE, uses a more complex system involving embeddings. We categorise these two approaches into \textbf{lexical} and \textbf{semantic} approaches to agentic-IR.

\subsection{Medical Code and Concept Retrieval}

\textbf{Medical codes} are digital ways of representing events or findings in the real world. A medical code might represent a diagnosis, such as diabetes type II, a laboratory finding or test result, or something more abstract, such as admission to hospital.

\textbf{Concepts} are abstractions of codes, aggregated at a lower level of granularity so as to be practically useful in informatics applications.
For instance, to answer the question `who in my practice has diabetes II?', we must have some `concept' of the codes which signify diabetes II. Table \ref{tab:concept-as-multi-vocab-codelist} shows four codes belonging to the diabetes II concept.
% \footnote{NB: SNOMED-CT refers to its codes as `concepts'.}

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
         Concept & Diabetes II \\
         \hline
         ICD-10 & E11.0, E11.1, ... \\
         SNOMED-CT & 44054006, 4130162, ...\\
    \end{tabular}
    \caption{Example medical \textit{codes} from the two most common vocabularies used in current medical practice, ICD-10 and SNOMED-CT, for the \textit{concept} Diabetes type II \cite{world_health_organization_icd-10_2004, bhattacharyya_introduction_2015}.}
    \label{tab:concept-as-multi-vocab-codelist}
\end{table}


Prior art for retrieving medical codes has tended to use non-agentic, retrieval pipelines based on semantic similarity \citet{baksi_medcoder_2024, ziletti_retrieval_2024}.
These systems operate at the level of medical codes, such as ICD-10 codes and their descriptions, whereas our system operates at a level higher: using internally defined `concepts' representing logical compositions of codes, such as `Cirrhosis and no liver transplant'.
These concepts are used to select cohorts of patients matching their criteria.

\textbf{Concept retrieval} is a critical sub-task upon which generating successful queries depends.
Retrieving appropriate concepts is more challenging than retrieving medical codes, since - unlike codes - we cannot rely on the LLM to have learnt concept names during the pre-training phase.
%Indeed concept names often require substantial local knowledge to understand.
% DIFFERENCES TO MEDICAL CODING:
%Mapping utterances to concepts requires handling of polysemy, synonymy, metonymy, and (overloaded) abbreviations, as well as local knowledge on naming conventions. Due to this complexity, 

Due to the ambiguity inherent in many natural language utterances, the agent often must act under imperfect information.
This leads to cases where the model changes it's interpretation of the user's utterance dynamically based on the content of it's information environment.
For example, the agent interprets \textit{`give me patients with CFS'} as a search for patients with \textbf{c}hronic \textbf{f}atigue \textbf{s}yndrome but changes it's interpretation upon seeing that all CFS concepts refer to the Clinical Frailty Scale. 

We experiment with a sub-agent for retrieving the correct concepts, a divide and conquer strategy in which the sub-agent retrieves entities and the main agent composes them into a query.
This simple task decomposition reduces the context poisoning which occurs when the main-agent performs a large number of searches, as the retrieval sub-agent only passes back relevant entities, instead of the many hundred concepts returned by lexical search (`diabetes' returns 407 concepts, for instance).

We demonstrate that this improves agent success rate (SR), particularly in complex multi-turn conversations in which the context contains a lot of information.
% We hypothesise that this is due to the irrelevant entities `distracting' the model via the attention mechanism, a hypothesis we provide evidence for through the inspection of attention-weights. % Is attention weight given to incorrect entities at the expense of correct ones!? Find out.

% We elect to use a lexical AIR approach, eschewing the complexity of creating and maintaining a semantic index of medical concepts.

% In a task-oriented setting, they implicitly perform the NLU, DST, policy, and generation aspects which previously would have been distinct modules.
% This change allows agents to dynamically interact with the environment and to change course as they progress and receive new information, enabling them to excel at complex tasks like programming where purely generative models typically fail.
% sole task is to retreive information, a paradigm described as `agentic information retrieval' by \citet{zhang_agentic_2025}.

\subsection{Health Informatics}
% Informatics
Health informatics is an interdisciplinary field at the intersection of data analytics and medicine, and entails improving the health outcomes through the broad use of clinical data. Informatics tasks include: writing database queries to select cohorts of patients meeting certain criteria, measuring the performance of interventions, and developing tools for clinicians to better access data. 

% EHR
The data used in healthcare informatics is stored in electronic healthcare records (EHR). Personal EHR are a rich store of information, often containing thousands of recorded medical codes dating back to a persons's birth. These records are used to great effect in large health organisations such as the NHS, where this data enables clinicians to proactively identify people's needs and to provide targeted interventions, through the use of digital clinical decision support systems (CDSS).


% Recent developments in data 
The quality of the recorded data has improved dramatically due to various incentive structures put in place, such as the mandate that providers and hospitals in the United States must exhibit `meaningful use' of the EHR in order to be eligible for Medicare and Medicaid funding, which has been fully in effect since 2016\footnote{https://www.healthit.gov/faq/what-meaningful-use}. In the UK similar incentives for good medical coding exist, with additional funds allocated to reimburse organisations which handle more complex cases, conditional on the diagnoses and procedures being properly coded.

% todo graphic of the EHR?

% Epidemiology
The ability of public health institutions to identify and respond to emergencies depends on their ability to aggregate and interpret information about the situation on the ground. Whilst digital records simplify the aggregation of said information, creating the correct database queries to analyse the data often requires the assistance of programmers, as CDSS tools typically provide limited analysis capabilities.


% TODO diagram of cohort selection (it's just a 2D space!) -- ref. the 3B1B video on Bayes' theorem.

We describe \textbf{HealthQueryAgent}, an agentic, task-oriented-dialogue (TOD) system for generating interpretable medical database queries using generative language models. We demonstrate the utility of this system through a beta deployment in NHS Nottingham and Nottinghamshire, where the system is used by members of both the clinical and analyst staff.
In our beta deployment, we present HealthQueryAgent via a chat interface embedded into an existing web platform for population health management, enabling the clinician to collaborate with the agent to refine the analysis. % todo: hard to parse the last sentence.

% Interpretability
For the results of an automated querying system to be actionable, they must first be interpretable. Specifically, generated database queries must be understandable to the user; whilst SQL is ubiquitous amongst analysts, clinicians will struggle to verify that an SQL query is correctly expressing their intent. To work around this limitation, we limit cohort selection queries (the predominant type of query) to a subset which can be expressed using a composition of only three logical operations: \texttt{AND, OR, AND NOT}. For clinician facing CDSS the tradeoff of expressivity for interpretability could be worthwile compared to text-to-SQL based systems, whose queries cannot be easily verified by non-programmers.

% is that our system generates queries in an interpretable intermediate representation (IR). In order to make decisions based on the results of the generated query, it is critical that the query itself can be checked logically by the user.In the case of an analyst generating SQL is appropriate, however non-programmers will have difficulty validating the correctness of SQL queries. 

 %clinicians can understand the underlying logic; something which is arguably not possible with text-to-sql pipelines used in other systems in the literature. We address this by expressing queries using a simple intermediate representation (IR) consisting of three first-order logic operations: \texttt{AND,OR,NOT} as well as the concept of a fraction. This IR is programmatically mapped to an SQL query and provides a simple mechanism for non-programmers to ensure that the query is logically correct- a key requirement for actionable analyses.

% Local context
In the context of the UK, where this study takes place, our National Health Service has announced three major shifts in it's operations\footnote{https://www.england.nhs.uk/digitaltechnology/the-single-patient-record/}:

\begin{enumerate}
	\item hospital to community
	\item sickness to prevention
	\item analogue to digital
\end{enumerate}

HealthQueryAgent fits squarely within these aims. By enabling primary (community) care physicians to proactively pin-point people who need assistance, the burden can be shifted away from expensive secondary care services (hospitals / inpatient facilities).

\section{Related Work}

Our proposed agent-based, task-oriented dialogue system for medical database querying sits at the intersection of three strands of research: Task-Oriented Dialogue, agent-based information retrieval, and text-to-SQL.

\subsection{Text-to-SQL}
Text-to-SQL is the task of operationalising a question by mapping it to a Structured Query Language (SQL) script. Medical applications of text-to-SQL have largely focussed on translating natural language queries into data warehouse queries as part of health data analyst workflows \citet{ziletti_retrieval_2024, ziletti_generating_2025}.
% One of the design goals of SQL was that non-programmers would be able to read and write queries \cite{chamberlin_early_2012}.

In this work, we generate an SQL query by first generating an intermediate representation which is then programmatically transformed into SQL, similar to in \cite{guo_towards_2019}.


\begin{figure}[t]
  \includegraphics[width=0.9\columnwidth]{content/flow_diagram.PNG}
	\caption{The flow of information in a dialogue with HealthQueryAgent (HQA). The agent generates database queries in a way which preserves privacy by design: no patient data ever leaves the secure environment, only medical concepts and other information required to \textit{construct} database queries.} 
  \label{fig:flow-diagram}
\end{figure}


\section{Method}
% A method is like a recipe.

HealthQueryAgent is an LLM based, end-to-end task-oriented dialogue system.
In this section, we describe it's components and how they integrate.

% \subsection{Automated testing} ...
\subsection{Retrieval Sub-Agent}
Identifying the appropriate medical concepts in a real-world clinical database is a challenging sub-task within HealthQueryAgent's workflows.
The task involves elements of semantic parsing and disambiguation, handling of polysemy, synonymy, metonymy, and abbreviations, as well as local knowledge on naming conventions.

The retrieval sub-agent is prompted to identify the top-$k$ most relevant concepts for a given NL expression, using two tools, one for searching concepts, and one for returning concepts by their IDs.
Returning concepts ends the sub-agents routine, and continues the main agent's trajectory with the newly-retrieved concepts.

To aid the model interpret highly specialised and evolving clinical language, the sub-agent may be provided with a third tool: `search\_umls', which enables it to retrieve information from the Unified Medical Language System® (UMLS®).
Information retrieved this way often includes mapping compound names/acronyms to drug names, e.g. `ddC' -> `dideoxycytidine' -> `Zalcitabine' (a drug used to treat HIV).
This improves the agent's SR on tasks involving niche knowledge.


\subsection{Workflows}

We provide abstract trajectories without specific named entities instead of in-context examples/exemplars.
% In-context learning (ICL) using examples or `exemplars' was once the main way to improve task-specific performance of LLMs we propose using abstract exemplars or `workflows' to achieve the same end.
Each abstract trajectory (workflow) describes a sequence of tool uses appropriate for a class of user request.
For instance the following is a workflow for computing the prevalence (rate) of a condition within a population:

\begin{enumerate}
	\item search(..., mode="concepts")
	\item create\_cohort(...)
	\item return\_rate\_query(...)
\end{enumerate}

Unlike ICL exemplars, workflows/templates do not reference specific entities, thereby avoiding biasing the generation process towards the inclusion of these irrelevant items, a problem faced in prior art.

\subsection{Tools}
The agent has a fixed set of tools (functions) provided in the industry standard json tools format.
The tools contain brief descriptions of their function (documentation), and are mapped 1:1 with Python functions internally, using a switch-case statement.
Tools were designed to be simple, easy to compose, and powerful. Empirically we observed that agents performance was largely dependant on the quality of tool documentation.

\subsubsection{Return}
The agent may return a result in the form of a URL link showing the user the result of the generated query, as well as the query itself.
The output modes include \texttt{patient\_list(cohort)}, \texttt{statistics(cohort)}, \texttt{performance\_indicator(cohort, KPI, location, comparison\_location)}.

\begin{table}[t]
\centering
\begin{tabular}{|p{2cm}|p{5cm}|}
\hline
	Output mode & Sample requests \\
\hline
\hline
	Patient List	& show me the \textcolor{blue}{dementia status} for people with moderate or severe frailty; \\
			& Hip fracture; \\ 
\hline
	Cohort Statistics 	& How many patients are on \textcolor{red}{PERT}? \\
				& What's the rate of ADHD in people with Autism? \\
				& Show me the distribution of \textcolor{teal}{ages} for \textcolor{orange}{dementia patients.} \\
\hline
	KPIs 	& How's \textcolor{brown}{Saxon Cross} doing on admissions avoidance? \\
		& Is Nottingham West doing better than East on diabetes management? \\	
\hline
\end{tabular}
\caption{
	Queries with named entities highlighted in various colours.
	There a number of matching entities types and the agent must infer from the query both the appropriate entity and the appropriate entitiy type.
	For instance for the phrase `\textcolor{blue}{dementia status}' is a request for a report (a specific columnar view of the patient list) but there also exist related
	concepts (e.g. \textcolor{orange}{Dementia}) and KPIS which the model must determine are irrelevant to the query despite their lexical similarity.
	\\ \textbf{Takeaway:} selecting entities to use the in the answer requires sophisticated interpretation of the user intent.  
}
\label{tab:sample-queries}
\end{table}

\subsubsection{Search}
To enable the agent to access the relevant contents of various tables, we provide it with a search tool.
The tool takes two arguments: a key, which is used for a simple substring match, and a mode, which is used to determine the type of entity being searched
for.

\subsubsection{Create Patient Cohort}
% todo
Creating select queries for cohorts of patients is a key function of HQA.
An example of a question requiring the user to create a cohort is shown in Figure \ref{fig:question-to-query}.:/
Much of the effort in optimising it's tools and abilities has been devoted to improving it's performance on this task.

%\enquote{
%Who is \textcolor{CornflowerBlue}{aged 75+} with \textcolor{Rhodamine}{moderate or severe frailty} in \textcolor{OliveGreen}{Nottingham West}, and who have either \textcolor{Maroon}{2+ emergency admissions in the last 12 months} or \textcolor{PineGreen}{polypharmacy of 5 or more medications?}
%}

%\begin{verbatim}
%Aged >=75 AND Frailty Score 6-8
%AND Nottingham West PCN
%AND (2+ emergency admissions L12M
%	OR Polypharmacy 5+ medications)

%\end{verbatim}



\begin{table}[t]
\centering
	\begin{tabular}{|p{2cm}|p{5cm}|}
\hline
	Entity Type & Sample entities \\

\hline
\hline
	Concepts 	& \textcolor{orange}{Dementia} \\
	 	        & \textcolor{red}{Pancreatic Enzyme Replacement Therapy} \\
			& Diabetes - L12M \\
			& 2+ hospital admissions L12M \\
			& Hip fracture \\ 
\hline
	Locations	& \textcolor{brown}{Saxon Cross General Practice;}\\	
			& Nottingham West; \\
			& Orchard Surgery; \\
\hline
	KPIs		& \% of diabetics whose HBa1c is 48mmol/mol or lower; \\
	 		& \% of medication reviews for those on the end of life register; \\	
\hline
	Reports		& \textcolor{blue}{Dementia:} \\
			& Age | Sex | MoCA score | \\
			& Frailty: \\
			& Age | Sex | Barthel independence score | clinical frailty score | history of falls \\
\hline
	Statistics	& \textcolor{teal}{Age bands} \\
			& Diagnoses \\
			& Admissions \\
\hline
\end{tabular}

\caption{
	A sample of entities which HealthQueryAgent needs to marshal in order to generate it's answer queries.
	`L12m' refers to data recorded in the the last 12 months.
	`HBa1c' is a blood sugar test used to measure the performance of diabetes treatments, the specific value corresponds to the clinical threshold for `well-controlled'.
%	\textbf{Takeaway}: the agent must retrieve local data to complete it's task.
}
	\label{tab:sample-entities}
\end{table}


\subsection{Data Protection} 
Ethically there is a responsibility to treat patient data as confidental and to share it strictly on a `need to know' basis.
Concepts pertaining to a certain record would be considered confidential information, even if the patient name and other identifiers were pseudonymised
\footnote{Under UK law, the information pertaining to a person is considered confidential even if it is anonymised.
\url{https://www.digitalregulations.innovation.nhs.uk/regulations-and-guidance-for-developers/all-developers-guidance/proof-of-concept-using-anonymous-or-artificial-health-data/}
}. 
To ensure that no confidential data can be accessed by the agent, we create a selection of database tools which do not provide access to the `Patient' table, or any tables derived from it; the agent is limited to generating a \textit{query} for patient information, which is executed by the user according to their resective level of permissions, as shown in Figure \ref{fig:system-diagram}.


\section{Experimental Setup}
Our framework is agnostic to the choice of LLM. We choose to evaluate using Gemini 2.5 Flash \citet{comanici_gemini_2025}, as Gemini 2.0 was identified as being the best-performing model in the closest prior art \citet{ziletti_generating_2025}.
Due to the iterative nature of prompt-development, we believe that our system and tool prompts are biased towards this model in a way that would make comparison to other models misleading, and so we report metrics only for Gemini 2.5 Flash.

We run a real-world evaluation in a dialogue based deployment of HQA, using 2.5 Flash in a zero-shot setup with `medium' thinking enabled, allowing the agent to reflect on the state of the conversation before generating it's utterance (text) and actions (tool-calls). A sample of 20 user requests are shown in Figure \ref{fig:sample-user-utterances}. We use a token limit of 128,000, approximately 183,000 words (token=0.7 words); a number which was chosen based on alpha-testing of the system.

\subsection*{Can we automatically map clinical questions to queries?}

To test this key hypothesis we evaluate two measures of success.
First, we measure the ability of our system to generate a query determined to be `useful' by the clinician or analyst, using labels assigned during real-world use shown in Table \ref{tab:user-feedback}.
Second, we get an experienced data analyst to grade 100 agent trajectories according to the rubric:
%Second, we construct a more controlled test in which we measure the agent's ability relative to two professional database analysts on a dataset of 100 questions sampled from our real-world dataset. We grade each agent according to the following two measures: 
\begin{enumerate}
	\item \textbf{parsing}: did the agent reasonably interpret the user query (to the extent possible)?
	\item \textbf{accuracy}: was the cohort of patients selected appropriate? 
\end{enumerate}


\subsection*{Can we maintain the agent's performance over long, multi-stage tasks, and between tasks?}

To test this we construct a serial benchmark of tasks that the agent is known to be capable of, and measure the relative performance degradation when the chat context contains unrelated task completions.

\subsection*{Does providing an external KB enhance the agent's ability to generate queries?}

To test this, we sample some of the rarer medical concepts available in the database and manually curate sets of synonyms and metonyms for those entities.
We compare the model's ability to complete tasks requiring these concepts with and without access to the UMLS KB as a resource.

\section{Results}
Our results are divided into two sections: one from our real-world user evaluation, and one from our evaluation using queries drawn from \citet{ziletti_generating_2025}.
% (descriptive) What did our experiments show?

\subsection{Cohort Query Evaluation}

\include{results_ziletti}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\hline
 & \multicolumn{2}{c}{Accuracy} & \multicolumn{4}{c}{Patient} & \\
\cmidrule(lr){2-3} \cmidrule(lr){4-7}

Standard & EX. & Semantic parsing & recall & precision &F1-score & Jaccard & Avg. Token use \\
\hline

\textsc{HQA} & \Accuracy{} (\Passed{} / \Total{}) & \textbf{\Semanticparsingjaccardscore} & \textbf{\Recall} & \Precision & \Fscore & \Jaccardscore & \Tokensmean \\

\textsc{+ IR Agent} & \irAccuracy{} (\irPassed{} / \irTotal{}) & \textbf{\irSemanticparsingjaccardscore} & \irRecall & \textbf{\irPrecision} & \irFscore & \irJaccardscore & \irTokensmean \\

\textsc{+ IR Agent w/ UMLS} & \textbf{\IRumlsAccuracy{} (\IRumlsPassed{} / \IRumlsTotal{})} & \IRumlsSemanticparsingjaccardscore & \IRumlsRecall & \IRumlsPrecision & \IRumlsFscore & \textbf{\IRumlsJaccardscore} & \textbf{\IRumlsTokensmean} \\
\hline

\multicolumn{8}{l}{Sequential} \\
\hline
\textsc{HQA} & \sAccuracy{} (\sPassed{} / \sTotal{}) & \sSemanticparsingjaccardscore & \sRecall & \sPrecision & \sFscore & \sJaccardscore & \sTokensmean \\
\textsc{+IR Agent} & \sirAccuracy{} (\sirPassed{} / \sirTotal{}) & \sirSemanticparsingjaccardscore & \sirRecall & \sirPrecision & \sirFscore & \sirJaccardscore & \sirTokensmean \\
\hline
% \textsc{HQA+IR Agent} & 41.67 & 0.88 & 0.88  &  0.69 & 0.74 & 0.68 & 37,075 \\
% \textsc{HQA+IR Agent + UMLS} & & tbc. -> & 0.88  &  0.69 & 0.74 & 0.68 & 37,075 \\

\end{tabular}}
\caption{
    HealthQueryAgent's (HQA) performance on epidemiological cohort questions published in \citet{ziletti_generating_2025}, with and without the Information Retrieval (IR) sub-agent, and with/without access to UMLS. Performance is measured relative to a reference answer created by a professional analyst. EX. = Execution accuracy (exact match).
}
\label{tab:results}
\end{table*}


\subsection{Real World User Evaluation}
This section contains results demonstrating the ability of HealthQueryAgent to answer population health questions in a real-world clinical database environment.

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
	              & User & Agent & Tool \\
\hline
	dialogues	& 108 & 108    & -   \\
	turns       & 178 & 178    & -   \\
    utterances	& 178 & 837    & 403   \\
	words 		& 2,221 & 16,892 & 315,429 \\
	est. tokens	& 3,184 & 24,322 & 450,612\\
\hline
\end{tabular}
\caption{
Summary statistics for HealthQueryAgent's conversations.
`Tool' indicates the result of a tool-use, e.g. a database search.
\textbf{Takeaway: the majority of words in the conversations were tool call results.}
}
\label{tab:conversation-statistics}
\end{table}

Figure \ref{tab:conversation-statistics} describes the conversations recorded in our real world user evaluation.
The number of words generated by the LLM seems to dominate that of the user, despite our generative LLM having `thinking' turned on enabling it to generate tokens outside of the conversation.
The number of words in the conversation from tool-calls dominates that of the user and agent combined by an OOM.

The number of turns is the same for user and agent messages, since the agent will respond to each user message.
The number of utterances is a count of discrete messages, e.g. interleaved text and tool-calls by the agent; for users this will be the same as the number of turns since a user cannot follow their own turn of speech.

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
	            & count & absolute  	&  relative \\	
\hline
	positive 	& 78	& 12.0\%		& 78.8\%	\\
% \hspace{0.5cm} +text 	& 5	& 0.2		& 0.1 \\
\hline
	negative 	& 21 	& 3.2\%      	& 21.2\%\\
% \hspace{0.5cm} +text	& 3	& 0.05		& 0.1	\\
\hline	
\end{tabular}
\caption{
	User labels annotated during the beta testing deployment.
	Users were prompted with the message `was this useful?' and a thumbsup / thumbsdown option denoting positive / negative feedback.
    \textbf{Takeaway: clinical users rate our system as being useful.}
}
\label{tab:user-feedback}
\end{table}


Table \ref{tab:user-feedback} shows that whilst the rate of feedback collection is low, when users rate the system they tend to rate it positively, assigning a positive label 78.8\% of the time.

To overcome the low absolute feedback rate, we collect data on which queries were run by the user.
% To ascertain the face-validity of a generated query, we check if the user accessed the result of that query.
This behaviour is a source of intrinsic labels: a user not executing a query suggests that they rejected the agent's implementation of their request/intent.
Of generated queries, the results of 70\% were accessed by the user, suggesting good face-validity of the agent trajectories to clinical and analyst staff.

% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|l|}
% \hline
% 	Agent performance & value \\
% \hline

% 	semantic parsing & -90\% 	\\
% 	cohort selection & -90\%	\\
% 	success rate	 & -85\%	\\
% 	% Cohen's $\kappa$ & -0.7 	\\
% \hline
% \end{tabular}
% \caption{
% 	Expert grading of the agent's performance on a subset of 100 randomly selected real-world queries.
% 	\textbf{Takeaway: analysts rated our agent as successfully parsing and operationalising user intents.}	
% }
% \label{tab:user-feedback}
% \end{table}

% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% 	semantic parsing  & cohort acc. & success rate	\\	
% \hline
% 	-90\%		  & -90\%		& -80\%	\\
% \hline	
% \end{tabular}
% \caption{
% 	Expert grading of the agent's performance on a subset of 100 randomly selected real-world queries.
% 	\textbf{Takeaway: analysts rated our agent as successfully parsing and operationalising user intents.}	
% }
% \label{tab:expert-feedback}
% \end{table}

% \begin{table}[h]
% \centering
% \begin{tabular}{|l|c|c|c|}
% \hline
% 				& \textbf{SR} & \textbf{Steps} & \textbf{Tokens} \\
% \hline
% HQA 				& \SR\% & -5 & -1,500,000 \\
% \hline
% HQA + sub-agent			& \SRRetrieval\% & -5 & -1,000,000 \\
% \hline
% HQA + sub-agent w/ UMLS		& \SRRetrievalUMLS\% & -6 & -1,000,000 \\
% \hline
% \end{tabular}
% 	\caption{Performance for HealthQueryAgent (HQA) with ablations for it's Retrieval sub-agent and the UMLS knowledge base on the development set.}
% 	\label{tab:results-for-single-queries}
% \end{table}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{content/pca_09_16_randomly_sampled_20.png}
  \caption{
    An interactive PCA visualisation of user messages.
  }
  \label{fig:sample-user-utterances}
\end{figure*}

\section{Conclusion}
% What did we take away from these results?
In this study we described HealthQueryAgent, a novel text-to-query system in which an agentic LLM determines how best to approach the problem itself rather than following a pre-determined pipeline.

We found evidence that information retrieval in real-world databases with semantically overlapping medical concepts was a challenging sub-task for LLMs; and that assigning this IR sub-task to a distinct sub-agent provided a performance uplift in our conversational dialogues setup.

We demonstrated that HQA can faithfully and interpretably generate health informatics queries, overcoming a limitation of prior work which noted that queries generated in SQL remain a `black box' to users unfamiliar with the language  (\citet{ziletti_generating_2025}).
Whilst in some cases this may be acceptable, the success of HQA is evidence that generating intermediate representations similar to that proposed by \citet{guo_towards_2019} is a practical approach to generating queries for users who may not be familiar with SQL.


\section{Limitations}
The limited grammar of our \textit{AND/OR} intermediate representation language limits the range of queries which can be performed by HQA.
For instance, custom age-ranges cannot be created, rather the model must rely on existing age-band concepts.


\section{Ethics Statement}
Population health management is a serious task, and the use of AI warrants proper ethical consideration.

To address the ethical concerns regarding this work, let us consider the mechanisms by which such a system could cause harm.

Since the agent cannot access patient data, we deem the risk of leaking PII to be limited to the uploading of patient information into the dialogue by a user and explicitly warn users at the beginning of each chat to not upload any PII.

The core risk we identify is that the system generates an incorrect output, leading to a clinician to be misled about the status of one or more patients, resulting in harm to those patient(s) due to the clinician taking a different course of action that they otherwise might, a risk which takes the following forms:

\begin{enumerate}
	\item Errors in selecting concepts
	\begin{enumerate}
		\item due to incorrect disambiguation e.g. of an overloaded terms
		\item due to lack of specialist knowledge e.g. multiple competing database concepts for the same utterance   
		\item due to under-specification of the user intent
		\item due to no appropriate concept existing 
		\item due to confabulation
		\item due to random chance
	\end{enumerate}
	\item Errors in formulating queries
	\begin{enumerate}
		\item due to semantic parsing errors 
		\item due to random chance
	\end{enumerate}
\end{enumerate}

\textbf{Errors in selecting concepts:}
Recall that clinical concepts refer to one or more medical codes, possibly with accompanying logic, such as a time-window: `COVID in the last six months' is a concept, for instance.

Errors in selecting concepts due to the presence of multiple retrieved concepts for a given search are a common error-mode of our system. Many of these concept selection sub-tasks require knowledge of the specific context in which a clinician or analyst created the concept in the first place, making it challenging for the LLM to select the correct one.

Additionally, LLMs have been known to confabulate/hallucinate when confronted with information they don't understand, something we have observed when handling highly-local knowledge internal to the NHS.

\textbf{Errors in formulating queries:}
Formulating the correct AND/OR query for a given utterance is of central importance, as it dictates the cohort of patients which will be selected.
Conditional on the correct concepts being selected and the user's intent being clear, we find that the tested LLMs consistently generate the correct logical queries.
In cases where the user intent is unclear, semantic parsing issues become more frequent. Due to the nature of LLMs, we cannot rule out these errors entirely; indeed, the fact that the LLM generates queries highly consistently may lull the user into a false sense of security, leading them to unthinkingly accept a result and thereby generating false knowledge. Our training material for users will note this fact and urge them to remain suspicious of the generated output.

\textbf{Ethics Summary:} we identify incorrect answers might lead to real-world harm. We address this concern by making the entire answer generation process transparent and interpretable, including the database queries which are generated in a simple, interpretable representation, enabling non-programmers to audit them.


\section*{Acknowledgments}
We thank the NHS Nottingham Data Management Team for their expertise and contributions to this project. We thank Anthony Dranfield and Ben Clarke for their assistance with the eHealthScope platform and database, and Carl Davis for organising the collaboration.

\bibliography{references}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}
% This is an appendix.

\end{document}
