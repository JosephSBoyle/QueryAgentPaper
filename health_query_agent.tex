\documentclass[11pt]{article}

% custom packages: %

\usepackage[dvipsnames]{xcolor}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{xurl}

\usepackage{standalone}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, fit}

\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%%

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{authblk}

\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{ClinQueryAgent: A Task-Oriented Dialogue System for Automated Clinical Informatics}
\title{ClinQueryAgent: A Conversational Agent for Clinical Informatics}

% Author information can be set in various styles:
% For several authors from the same institution:

%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{ \bf
Joseph S. Boyle$^{1,2,7}$ 
Mike O'Neil$^{6}$
Maria Liakata$^{2,4,5}$
Alison Q. Smithard$^{1,3,7}$
\\ \\

$^1$Canon Medical Research Europe $^2$Queen Mary University of London \\ $^3$University of Edinburgh $^4$University of Warwick \\
$^5$The Alan Turing Institute $^6$Nottingham and Nottinghamshire ICB, NHS 
}
% \small{
%    \textbf{Correspondence:} \href{mailto:joseph.boyle@mre.medical.canon}{joseph.boyle@mre.medical.canon}
%  }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}

% load results values
\include{results_development}
\include{results_ziletti}

\maketitle
\begin{abstract}
% What does this paper add
In this paper we introduce \textsc{ClinQueryAgent}, a dialogue agent for automating population clinical informatics queries.
We deploy the system via a chat window embedded within an existing population health management platform and run experiments to determine its utility to a diverse userbase of 2,000 analysts and clinicians querying the data of 1,279,215 active patients.
We evaluate the system's capacity to autonomously handle a range of health informatics tasks on the \textit{EpidemiologicalCohort} benchmark and through a real-world user evaluation.
Our results show that, using natural language requests that require minimal clinical coding expertise, both analysts and clinicians are able to easily generate actionable information from patient health records.
% can act as clinical copilots, aiding users in performing specialist tasks (database querying) in a way that maintains transparency.

% We compare a range of frontier LLMs on a benchmark comprised of real-world user (question, query) pairs, and find (TBD). % todo

% Analysing healthcare data can be an arduous and error-prone task when done manually. 
% Three examples are: listing patients who meet some criteria; calculating the rate of a condition within a subpopulation; and measuring the performance of their organisation according to some metrics.

% The aim of population health analysis is for a user to generate some knowledge from a database by conducting a database query to access information regarding a set of specified patients. 
%Specialist analysts are needed to translate clinical queries into database ones, imposing a steep cost and introducing a considerable delay. We hypothesise a system for automatically performing this translation in an interpretable fashion could drastically reduce both the cost and the latency to performing population health analysis, and enable clinicians to rapidly generate actionable knowledge from clinical data.

% This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style files for ACL conferences.
% The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
% These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\begin{figure}[htbp]
\centering
\includestandalone{figure\_one}
\caption{
\colorbox{yellow!17}{Query Agent} parses natural language questions into database queries in an agentic loop. % dynamically determining the course of action as it interacts with its data environment.
The data environment comprises a local database of analyst-created medical \colorbox{red!17}{concepts}, and an open-access knowledge-base, \colorbox{blue!10}{UMLS}, which is used to help ground the agent faithfully map clinical jargon.
% 	By looking up `ddC', the agent is able to link it to the brand-name Zalcitabine, . % a process which involves multi-step information retrieval.
The main agent hands off the task of finding Concepts to a \colorbox{teal!10}{Retrieval Agent} so that it can focus on creating the database query, improving it's success rate.
}
\label{fig:question-to-query}

\end{figure}

\section{Introduction}

% focus has been elsewhere
Healthcare organisations collect large quantities of data, which clinicians can use to proactively manage the needs of their patients. However, the task of interrogating recorded healthcare data is complex; to perform sophisticated queries, clinicians rely on specialist data analysts.

Whilst there has been a concerted effort from machine learning researchers to automate data generation tasks such as report writing and clinical coding, relatively few studies have investigated how to maximise clinical benefit from recorded data.

% Ensure this is rendered on page 2.

% \begin{figure*}[t]
%   \includegraphics[width=\linewidth]{content/system_diagram3.png}
% 	\caption{System diagram showing a subset of QueryAgent's tools. The agent must enact multi-step plans using a concept database and a medical knowledge base to create an analysis query, which is then passed to the user who will be able to view the retrieved results. To ensure privacy, the agent's tools cannot access the Patient EHR database. The user may interactively refine the query by conversing with the agent, for instance prompting it to modify its results or asking follow up questions.}
% 	\label{fig:system-diagram}
% \end{figure*}


% Formulation
% \subsection{Task}
This paper aims to directly generate population health database queries from real-world user questions, enabling clinicians to use natural language requests to query electronic health record (EHR) data.
% Text-to-query systems rely on a range of NLU capabilities. Semantic parsing is required to interpret the user's intent, biomedical NER to determine the concepts being referenced, 
% This involves a number of challenges: determining the user's intent (semantic parsing); disambiguating complex and often overloaded medical language; retrieving and selecting the appropriate medical concepts; and finally creating a query.
We make the following contributions:

\begin{enumerate}

\item We demonstrate that conversational agents can provide accurate natural language interfaces to clinical databases in an interpretable and privacy-preserving manner.

\item We observe that accuracy degrades and cost increases over the course of multi-turn conversations due to the long context arising from the database search results, and hence propose use of a second subordinate agent to handle database concept searches. % enabling agents to maintain coherence over long

\item We demonstrate that providing access to a clinical ontology including synonyms for each concept and relationships between concepts, namely the \emph{Unified Medical Language System (UMLS)}, improves performance for queries involving niche medical concepts.
\end{enumerate}

% Does providing an external knowledge base (KB) enhance the agent's ability to generate queries? Prior studies show that LLMs perform worse on rarer medical concepts. Does an external KB benefit queries for rare medical concepts more than common ones?

\subsection{Task-Oriented Dialogue and LLM Agents}
% todo: make more relevant to our work.
In Task-Oriented Dialogues (TODs), an agent and user collaborate to act out a user's goal.
Core to successful TOD is semantic parsing (i.e. the process of creating a logical interpretation of an utterance), which must take into account the dialogue thus far (i.e. context) in order to determine what action the agent should perform next.

Traditional TOD systems typically consist of a pipeline of distinct modules, each with a specific responsibility.
A modular architecture might include a natural language understanding (NLU) component for understanding the user's intent and extracting relevant structured information; a dialogue state tracking (DST) component for monitoring the evolving state of the extracted information (DST); a policy module for determining the next action based on the current state; and a natural language generation (NLG) model for synthesising a response. Performance in these systems is hampered by the propagation of errors between modules (\cite{yi_survey_2025}).

More recent TOD systems use a single sequence-to-sequence model to perform the entire task. For instance, \citet{wen_network-based_2017} developed an end-to-end trainable neural network architecture. \citet{andreas_task-oriented_2020} were the first to treat TOD as a data flow graph in which the agent iteratively generates a program as the conversation progresses.

The advent of powerful pre-trained large language models (LLMs) has enabled TOD systems in which the LLM acts as the agent without task-specific training. In 2022, \citet{hosseini-asl_simple_2022} created SimpleTOD, whereby GPT-2 performs all aspects of task completion including database calls, with the results concatenated to the context. Subsequent systems have continued to function via autoregressive language modelling, interleaving reasoning traces with task-specific actions, and interacting with external tools by issuing \emph{tool calls}, under the so-called `ReAct' paradigm \cite{yao_reac_2023}.
% Two key challenges in TOD maintaining coherence over long multi-turn conversations (\citet{laban_llms_2025}), as well as identifying when f under-specified questions\cite{gan_clarq-llm_2024}
Using this paradigm, modern LLMs like Claude Code (Anthropic), Gemini-CLI (Google) and Codex (OpenAI) are capable of autonomously performing tasks which take human engineers multiple hours to complete \cite{kwa_measuring_2025}. 
%LLM agents (agents) refer to instruction-tuned models which are augmented with tools (functions) which enable them to take actions within an environment e.g. retrieving information from Wikipedia in order to obtain an answer \cite{yao_reac_2023}.


To conclude, LLM agents are a new and powerful approach to goal-oriented dialogue systems, in which the agent can autonomously and iteratively interact with its environment in order to accomplish the user's goals.


\subsection{Agentic Information Retrieval}
Agentic information retrieval (AIR) is a new paradigm in which language models iteratively interact with the environment in the `ReAct' loop to \emph{retrieve information} \citet{zhang_agentic_2025}.
The key of an agentic approach to IR over a traditional one is the ability to adapt their search as information is collected.
%This makes the agentic approach more robust to various linguistic phenomena.

A key design decision for Agentic AI is how they should perform information retrieval.
The current SOTA coding agent, Claude Code, uses only a lexical search function akin to UNIX's \textitt{grep} to retrieve source files whilst Windsurf, a popular agentic coding IDE, uses a more complex system involving vector embeddings.
We categorise these two approaches into \textbf{lexical} and \textbf{semantic} approaches to agentic-IR.
In this work we opt for a lexical approach, removing the complexity of creating and maintaining vector indexes.


\subsection{Medical Codes and Concepts}

\textbf{Codes} are digital ways of representing events or findings in the real world. A medical code might represent a diagnosis, such as diabetes type II, a laboratory finding or test result, or something more abstract, such as admission to hospital.

\textbf{Concepts} are abstractions of codes, aggregated at a coarser level of granularity so as to be practically useful in informatics applications.
For instance, to answer the question `who in my practice has diabetes II?', we must have some `concept' of the many codes which indicate diabetes II. Table \ref{tab:concept-as-multi-vocab-codelist} shows four codes belonging to the diabetes II concept.
% Unlike codes vocabularies which are developed and maintained by a central body- such as the WHO for ICD-10- concepts are often developed in-house.

% \footnote{NB: SNOMED-CT refers to its codes as `concepts'.}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
         \hline
         ICD-10 & E11.0, E11.1, ... \\
         SNOMED-CT & 44054006, 4130162, ...\\
         \hline
    \end{tabular}
    \caption{Example medical \textit{codes} from the two most common vocabularies used in current medical practice, ICD-10 and SNOMED-CT, for the \textit{concept} Diabetes type II \cite{world_health_organization_icd-10_2004, bhattacharyya_introduction_2015}.}
    \label{tab:concept-as-multi-vocab-codelist}
\end{table}


Prior art for retrieving medical codes has tended to use retrieval pipelines based on semantic similarity \citet{baksi_medcoder_2024, ziletti_retrieval_2024}.
These systems operate at the level of medical codes, such as ICD-10 codes and their descriptions, whereas our system operates at a level higher: using internally defined `concepts' representing logical compositions of codes, such as `Cirrhosis and no liver transplant'.
These concepts are used to select cohorts of patients matching their criteria.

\textbf{Concept retrieval} is a critical sub-task upon which generating successful queries depends.
Retrieving appropriate concepts is more challenging than retrieving medical codes, since unlike codes we cannot rely on the LLM to have learnt concept names during the pre-training phase.
%Indeed concept names often require substantial local knowledge to understand.
% DIFFERENCES TO MEDICAL CODING:
%Mapping utterances to concepts requires handling of polysemy, synonymy, metonymy, and (overloaded) abbreviations, as well as local knowledge on naming conventions. Due to this complexity, 

Due to the ambiguity inherent in many natural language utterances, the agent often must act under imperfect information.
This leads to cases where the model changes it's interpretation of the user's utterance dynamically based on the content of it's information environment.
For example, the agent interprets \textit{`give me patients with CFS'} as a search for patients with \textbf{c}hronic \textbf{f}atigue \textbf{s}yndrome but changes it's interpretation upon seeing that all CFS concepts refer to the Clinical Frailty Scale. 


The degradation of LLM outout quality in long, multi-turn conversations has been noted in prior art \cite{laban_llms_2025}, and is colloquially referred to as `context rot' by practitioners.
We experiment with a sub-agent for retrieving the correct concepts, a divide and conquer strategy in which the sub-agent retrieves entities and the main agent composes them into a query.
This simple task decomposition reduces the context degradation which occurs when the main-agent performs a large number of searches, as the retrieval sub-agent only passes back a small number of relevant entities, acting as a filter for the large number of concepts returned by lexical search methods (`diabetes' returns 407 concepts, for instance).

% We hypothesise that this is due to the irrelevant entities `distracting' the model via the attention mechanism, a hypothesis we provide evidence for through the inspection of attention-weights. % Is attention weight given to incorrect entities at the expense of correct ones!? Find out.

% We elect to use a lexical AIR approach, eschewing the complexity of creating and maintaining a semantic index of medical concepts.

% In a task-oriented setting, they implicitly perform the NLU, DST, policy, and generation aspects which previously would have been distinct modules.
% This change allows agents to dynamically interact with the environment and to change course as they progress and receive new information, enabling them to excel at complex tasks like programming where purely generative models typically fail.
% sole task is to retreive information, a paradigm described as `agentic information retrieval' by \citet{zhang_agentic_2025}.

% \subsection{Health Informatics}
% Informatics
% Health informatics is an interdisciplinary field at the intersection of data analytics and medicine, and entails improving the health outcomes through the broad use of clinical data.
% In this paper, we tackle the informatics task of creating database
% Informatics tasks include: writing database queries to select cohorts of patients meeting certain criteria, measuring the performance of interventions, and developing tools for clinicians to better access data. 

% EHR
% The data used in healthcare informatics is stored in electronic healthcare records (EHR).
% Personal EHR are a rich store of information, often containing thousands of recorded medical codes dating back to a person's birth.
% These records are used to great effect in large health organisations such as the NHS, where this data enables clinicians to proactively identify people's needs and to provide targeted interventions, through the use of digital clinical decision support systems (CDSS).


% Recent developments in data 
%The quality of the recorded data has improved dramatically due to various incentive structures put in place, such as the mandate that providers and hospitals in the United States must exhibit `meaningful use' of the EHR in order to be eligible for Medicare and Medicaid funding, which has been fully in effect since 2016\footnote{https://www.healthit.gov/faq/what-meaningful-use}. In the UK similar incentives for good medical coding exist, with additional funds allocated to reimburse organisations which handle more complex cases, conditional on the diagnoses and procedures being properly coded.

% todo graphic of the EHR?

% Epidemiology
%The ability of public health institutions to identify and respond to emergencies depends on their ability to aggregate and interpret information about the situation on the ground. Whilst digital records simplify the aggregation of said information, creating the correct database queries to analyse the data often requires the assistance of programmers, as CDSS tools typically provide limited analysis capabilities.


% TODO diagram of cohort selection (it's just a 2D space!) -- ref. the 3B1B video on Bayes' theorem.

\subsection{\textsc{QueryAgent}}
We describe QueryAgent, an agentic, task-oriented-dialogue (TOD) system for generating interpretable medical database queries using generative language models. We demonstrate the utility of this system through a beta deployment in NHS Nottingham and Nottinghamshire, where the system is used by members of both the clinical and analyst staff.
In our beta deployment, we present QueryAgent via a chat interface embedded into an existing web platform for population health management, enabling the clinician to collaborate with the agent to refine the analysis. % todo: hard to parse the last sentence.

% Interpretability
For the results of an automated querying system to be actionable, they must first be understandable.
Structured Query Language (SQL) is not easy to understand for clinicians most of whom are not programmers, and so we elect to represent our queries in a simpler grammar comprising only three logical operations: \texttt{AND, OR, AND NOT}.
This choice enables non-programmers to verify the correctness of queries in semantics similar to plain English.
% that an SQL query is correctly expressing their intent. To work around this limitation, we limit cohort selection queries (the predominant type of query) to a subset which can be expressed using a composition of only three logical operations: \texttt{AND, OR, AND NOT}. For clinician facing CDSS the tradeoff of expressivity for interpretability could be worthwile compared to text-to-SQL based systems, whose queries cannot be easily verified by non-programmers.

% is that our system generates queries in an interpretable intermediate representation (IR). In order to make decisions based on the results of the generated query, it is critical that the query itself can be checked logically by the user.In the case of an analyst generating SQL is appropriate, however non-programmers will have difficulty validating the correctness of SQL queries. 

 %clinicians can understand the underlying logic; something which is arguably not possible with text-to-sql pipelines used in other systems in the literature. We address this by expressing queries using a simple intermediate representation (IR) consisting of three first-order logic operations: \texttt{AND,OR,NOT} as well as the concept of a fraction. This IR is programmatically mapped to an SQL query and provides a simple mechanism for non-programmers to ensure that the query is logically correct- a key requirement for actionable analyses.

%% Local context
% In the context of the UK, where this study takes place, the NHS has announced three major shifts in it's operations\footnote{https://www.england.nhs.uk/digitaltechnology/the-single-patient-record/}:

%\begin{enumerate}
%	\item hospital to community
%	\item sickness to prevention
%	\item analogue to digital
%\end{enumerate}

%QueryAgent fits squarely within these aims. By enabling primary (community) care physicians to proactively pin-point people who need assistance, the burden can be shifted away from expensive secondary care services (hospitals / inpatient facilities).

\section{Related Work}

Our proposed agent-based, task-oriented dialogue system for medical database querying sits at the intersection of three strands of research: Task-Oriented Dialogue, agent-based information retrieval, and text-to-SQL.

\subsection{Text-to-SQL}
Text-to-SQL is the task of operationalising a question by mapping it to a Structured Query Language (SQL) script. Medical applications of text-to-SQL have largely focussed on translating natural language queries into data warehouse queries as part of health data analyst workflows \citet{ziletti_retrieval_2024, ziletti_generating_2025}.
% One of the design goals of SQL was that non-programmers would be able to read and write queries \cite{chamberlin_early_2012}.

In this work, we generate an SQL query by first generating an intermediate representation which is then programmatically transformed into SQL, similar to in \cite{guo_towards_2019}.


% \begin{figure}[t]
%   \includegraphics[width=0.9\columnwidth]{content/flow_diagram.PNG}
% 	\caption{The flow of information in a dialogue with QueryAgent (QA). The agent generates database queries in a way which preserves privacy by design: no patient data ever leaves the secure environment, only medical concepts and other information required to \textit{construct} database queries.} 
%   \label{fig:flow-diagram}
% \end{figure}
\begin{figure*}[htbp]
	\centering
	\includestandalone[width=0.9\textwidth]{figure\_two}
	\caption{
Query Agent (QA) is situated within the NHS Intranet and can access data from a locally created Concept database, as well as the world wide web.
Data is aggregated from inpatient/outpatient and specialist care centres in a single warehouse. Patient privacy is fully preserved and no information from EHRs is ever accessible to the CQA, rather it generates a \textit{query} which is executed by the user, allowing them to view data according to their level of permissions.
}
	\label{fig:system-diagram}
\end{figure*}

\subsection{Medical Agents}
\citep{wang_survey_2025} provide a survey of 20 distinct task applications of agents in healthcare, with identified tasks including diagnosing patients and generating documentation.

\citep{jiang_medagentbench_2025} created MedAgentBench environment for measuring the ability of agents to perform operations on a patient database from NL instructions.
Many of the questions are similar to those performed in our paper, though the benchmark does not test the retrieval of medical concepts, which are provided for the model in the prompt.
They provide evidence that when given the relevant codes/concepts, pre-trained LLMs demonstrate good capacity to complete various tasks such as computing patient statistics and so on.
This forms part of our motivation to experiment with with an information retrieval agent that can agentically retrieve the relevant concepts, allowing QueryAgent to focus on creating the correct query. 

Closely related to the concept retrieval subtask is work applying medical agents to medical coding (A.K.A clinical-coding; ICD-coding). The term `agent' in these applications has been applied to refer to agents which are instructed to act as particular actors in the code selection pipeline \cite{li_exploring_2024,motzfeldt_code_2025}, which \cite{schulhoff_prompt_2024} taxonomise as a prompting strategy.
Whilst \citet{motzfeldt_code_2025} describe an agentic system as being comprised of `modular components that can be implemented in various ways', we believe that this might better be described as a multi-role pipeline than an agent.
What distinguishes agents from these pipelines is the ability to determine the next step in the generation process dynamically, in response to the evolving information environment (\citet{zhang_agentic_2025}).

\section{Method}
% A method is like a recipe.

QueryAgent is an LLM based, end-to-end task-oriented dialogue system.
In this section, we describe it's components and how they integrate.

% \subsection{Automated testing} ...
\subsection{Retrieval Sub-Agent: \textsc{IR Agent}}
Identifying the appropriate medical concepts in a real-world clinical database is a challenging sub-task within QueryAgent's workflows.
The task involves elements of semantic parsing and disambiguation, handling of polysemy, synonymy, metonymy, and abbreviations, as well as local knowledge on naming conventions.

The retrieval sub-agent is prompted to identify the top-$k$ most relevant concepts for a given NL expression, using two tools, one for searching concepts, and one for returning concepts by their IDs.
Returning concepts ends the sub-agents routine, and continues the main agent's trajectory with the newly-retrieved concepts.

To aid the model link clinical jargon to concepts,  the sub-agent may be provided with a `search\_umls' tool, which enables it to retrieve information from the Unified Medical Language System® (UMLS®).
Information retrieved this way often includes mapping chemical compounds to drug names, e.g. `ddC' -> `dideoxycytidine' -> `Zalcitabine' (a drug used to treat HIV).
This improves the agent's SR on tasks involving niche knowledge, as shown in Figure \ref{fig:question-to-query}.

\subsection{Workflows}

We provide abstract trajectories without specific named entities instead of in-context examples/exemplars.
% In-context learning (ICL) using examples or `exemplars' was once the main way to improve task-specific performance of LLMs we propose using abstract exemplars or `workflows' to achieve the same end.
Each trajectory (workflow) describes a sequence of tool uses appropriate for a class of user request.
For instance the following is a workflow for computing the prevalence (rate) of a condition within a population:
\begin{verbatim}
1. retrieve(...)      # get concepts
2. create_cohort(...) # numerator
3. create_cohort(...) # denominator
4. return(...)
\end{verbatim}

Unlike ICL exemplars, workflows/templates do not reference specific entities, thereby avoiding biasing the generation process towards the inclusion of these irrelevant items, a problem faced in prior art.

\subsection{Tools}
The agent has a fixed set of tools (functions) provided in the industry standard json tools format.
Tools were designed to be simple, easy to compose, and powerful.
Empirically we observed that agents performance was largely contingent on the quality of tool documentation.

\subsubsection*{\texttt{return(cohort: CohortID)}}
The agent may return a result in the form of a URL link showing the user the result of the generated query, as well as the query itself.
The output modes include \texttt{patient\_list}, showing the user the cohort, and \texttt{patient\_statistics}, summarising the cohort.

% \begin{table}[t]
% \centering
% \begin{tabular}{|p{2cm}|p{5cm}|}
% \hline
% 	Question Category & Examples \\
% \hline
% \hline
% 	Cohort List	& show me the \textcolor{blue}{dementia status} for people with moderate or severe frailty \\
% 			& what are the comorbidities of people who take Zalcitabine \\ 
% \hline
% 	Cohort Statistics 	& How many patients are on \textcolor{red}{PERT}? \\
% 				& What's the rate of ADHD in people with Autism? \\
% 				& Show me the distribution of \textcolor{teal}{ages} for \textcolor{orange}{dementia patients.} \\
% \hline
% 	KPIs 	& How's \textcolor{brown}{Saxon Cross} doing on admissions avoidance? \\
% 		& Is Nottingham West doing better than East on diabetes management? \\
% \hline
% \end{tabular}
% \caption{
% 	Queries with named entities highlighted in various colours.
% 	There a number of matching entities types and the agent must infer from the query both the appropriate entity and the appropriate entity type.
% 	For instance for the phrase `\textcolor{blue}{dementia status}' is a request for a report (a specific columnar view of the patient list) but there also exist related
% 	concepts (e.g. \textcolor{orange}{Dementia}) and KPIS which the model must determine are irrelevant to the query despite their lexical similarity.
% 	\\ \textbf{Takeaway:} selecting entities to use the in the answer requires a contextual interpretation of the user intent.  
% }
% \label{tab:sample-queries}
% \end{table}

\subsubsection*{\texttt{retrieve(term: str) -> list[Concept]}}
This tool enables the agent to invoke the IR sub-agent to find the relevant concepts for the given term.
For instance, \texttt{retrieve(`diabetes')} would return `Diabetes I, Diabetes II, Suspected Diabetes' and so on.

\subsubsection*{\texttt{search(key: str) -> list[Concept]}}
To enable the sub-agent to access the relevant contents of various tables, we provide it with a search tool.
The tool takes two arguments: a key, which is used for a simple substring match, and a mode, which is used to determine the type of entity being searched
for.

\subsubsection*{\texttt{create\_cohort(tree: str) -> CohortID}}
% todo
Creating select queries for cohorts of patients is a key function of QA.
An example of a question requiring the user to create a cohort is shown in Figure \ref{fig:question-to-query}.

%\enquote{
%Who is \textcolor{CornflowerBlue}{aged 75+} with \textcolor{Rhodamine}{moderate or severe frailty} in \textcolor{OliveGreen}{Nottingham West}, and who have either \textcolor{Maroon}{2+ emergency admissions in the last 12 months} or \textcolor{PineGreen}{polypharmacy of 5 or more medications?}
%}

%\begin{verbatim}
%Aged >=75 AND Frailty Score 6-8
%AND Nottingham West PCN
%AND (2+ emergency admissions L12M
%	OR Polypharmacy 5+ medications)

%\end{verbatim}



% \begin{table}[t]
% \centering
% 	\begin{tabular}{|p{2cm}|p{5cm}|}
%\hline
%	Entity Type & Sample entities \\

%\hline
%\hline
%	Concepts 	& \textcolor{orange}{Dementia} \\
%	 	        & \textcolor{red}{Pancreatic Enzyme Replacement Therapy} \\
%			& Diabetes - L12M \\
%			& 2+ hospital admissions L12M \\
%			& Hip fracture \\ 
%\hline
%	Locations	& \textcolor{brown}{Saxon Cross General Practice;}\\	
%			& Nottingham West; \\
%			& Orchard Surgery; \\
%\hline
%	KPIs		& \% of diabetics whose HBa1c is 48mmol/mol or lower; \\
%	 		& \% of medication reviews for those on the end of life register; \\	
%\hline
%	Reports		& \textcolor{blue}{Dementia:} \\
%			& Age | Sex | MoCA score | \\
%			& Frailty: \\
%			& Age | Sex | Barthel independence score | clinical frailty score | history of falls \\
%\hline
%	Statistics	& \textcolor{teal}{Age bands} \\
%			& Diagnoses \\
%			& Admissions \\
%\hline
%\end{tabular}

%\caption{
%	A sample of entities which QueryAgent needs to marshal in order to generate it's answer queries.
%	`L12m' refers to data recorded in the the last 12 months.
%	`HBa1c' is a blood sugar test used to measure the performance of diabetes treatments, the specific value corresponds to the clinical threshold for `well-controlled'.
%%	\textbf{Takeaway}: the agent must retrieve local data to complete it's task.
%}
%	\label{tab:sample-entities}
%\end{table}


\section{Experimental Setup}
Our framework is agnostic to the choice of LLM. We choose to evaluate using Gemini 2.5 Flash \citet{comanici_gemini_2025}, as Gemini 2.0 was identified as being the best-performing model in the closest prior art \citet{ziletti_generating_2025}.
Due to the iterative nature of prompt-development, we believe that our system and tool prompts are biased towards this model in a way that would make comparison to other models misleading, and so we report metrics only for Gemini 2.5 Flash.

We run a real-world evaluation in a dialogue based deployment of QA, using 2.5 Flash in a zero-shot setup with `medium' thinking enabled, allowing the agent to reflect on the state of the conversation before generating it's utterance (text) and actions (tool-calls). A sample of 20 user requests are shown in Figure \ref{fig:sample-user-utterances}. We use a token limit of 128,000, approximately 183,000 words (token=0.7 words); a number which was chosen based on alpha-testing of the system.

\subsection*{Can we automatically map clinical questions to queries?}

We evaluate two measures of success.
First, we measure the ability of our system to generate a query determined to be `useful' by the clinician or analyst, using labels assigned during real-world use shown in Table \ref{tab:user-feedback}.
Second, we create a benchmark using epidemiological cohort questions form \citet{ziletti_generating_2025}.

% Second, we get an experienced data analyst to grade 100 agent trajectories according to the rubric:
%Second, we construct a more controlled test in which we measure the agent's ability relative to two professional database analysts on a dataset of 100 questions sampled from our real-world dataset. We grade each agent according to the following two measures: 
% \begin{enumerate}
%	\item \textbf{parsing}: did the agent reasonably interpret the user query (to the extent possible)?
%	\item \textbf{accuracy}: was the cohort of patients selected appropriate? 
%\end{enumerate}


\subsection*{Can we maintain the agent's performance over long, multi-stage tasks, and between tasks?}

To test this, we evaluate the performance of agents in an atomic (single-question) vs shared (multi-question) context, using questions from the EpiCoho
dataset.

% \subsection*{Does providing an external KB enhance the agent's ability to generate queries?}
%To test this, we sample some of the rarer medical concepts available in the database and manually curate sets of synonyms and metonyms for those entities.
% We compare the model's ability to complete tasks requiring these concepts with and without access to the UMLS KB as a resource.

\section{Results}

\subsection{Epidemiological Cohort Queries}

Table \ref{tab:epi-coho-results} shows the performance on the \textit{Epi Coho} questions created by \citet{ziletti_generating_2025}, evaluated against reference answers created by an analyst.
% Semantic parsing is measure as the correct interpretation of the question, as measured by the Jaccard index of the given return arguments.
Patient-level recall is the proportion of the patients in the reference answer cohort who were in the generated cohort; precision is the proportion of patients in the generated cohort who were in the reference cohort; patient F1-score is the harmonic mean of these two metrics.


\begin{table*}[htb!]
\centering
\resizebox{\textwidth}{!}{
	\begin{tabular}{lcccr}
		& \multicolumn{3}{c}{Patient Overlap} & Avg. tokens \\
		\cmidrule(lr){2-4}
		Single Questions & Recall & Precision & F1-score & used (\downarrow) \\ 
		\hline
		\textsc{QueryAgent} & \Recall & \Precision & \Fscore & \Tokensmean \\
		
		\textsc{+ RetrievalAgent} &  \textbf{\irRecall} & \irPrecision & \textbf{\irFscore} & \textbf{\irTokensmean} \\
		
		\textsc{+ RetrievalAgent} + UMLS & \irumlsRecall & \irumlsPrecision & \irumlsFscore & \irumlsTokensmean \\
		\hline
		\multicolumn{7}{l}{Chained Questions} \\
		\hline
		\textsc{QueryAgent} & \sRecall & \sPrecision & \sFscore & \sTokensmean \\
		\textsc{+RetrievalAgent} & \textbf{\sirRecall} & \textbf{\sirPrecision} & \textbf{\sirFscore} & \sirTokensmean \\
		\textsc{+RetrievalAgent} + UMLS  & \sirumlsRecall & \sirumlsPrecision & \sirumlsFscore & \textbf{\sirumlsTokensmean}
		\\
		\hline
	\end{tabular}
}
\caption{
    QueryAgent's (QA) performance on 35 epidemiological cohort questions, with and without the Information Retrieval (IR) sub-agent, and with/without access to UMLS. We assess accuracy as a) whether or not the retrieved dataset exactly matches the expected dataset b) whether the type of query is correct, e.g. a count of patients versus a list of patients.
	Performance is automatically evaluated using an analyst created answer as ground-truth and averaged at the query level.
	The chained questions setup asks questions one after the other, within the same conversation.
	% The addition of the IR sub-agent enables the system to maintain good performance over a greater number of questions, suffering only a small performance penalty in comparison to the QueryAgent only setup.
}

\label{tab:epi-coho-results}
\end{table*}



\subsection{Real World Queries}
This section contains results demonstrating the ability of QueryAgent to answer population health questions in a real-world clinical environment.

% TODO count number of chats with feedback...
Table \ref{tab:user-feedback} shows that whilst users tend to rate the system positively, assigning a positive label at roughly 4x the rate of negative lables.
Of generated queries, the results of 70\% were accessed by the user, suggesting good face-validity of the agent trajectories to clinical and analyst staff.

%To overcome the low absolute feedback rate, we collect data on which queries were run by the user.
% To ascertain the face-validity of a generated query, we check if the user accessed the result of that query.
%This behaviour is a source of intrinsic labels: a user not executing a query suggests that they rejected the agent's implementation of their request/intent.


\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|}
\hline
	            & count &  proportion \\	
\hline
	positive 	& 78 & 78.8\%	\\
% \hspace{0.5cm} +text 	& 5	& 0.2		& 0.1 \\
\hline
	negative 	& 21 & 21.2\%\\
% \hspace{0.5cm} +text	& 3	& 0.05		& 0.1	\\
\hline
\end{tabular}
\caption{
	User labels annotated during the beta testing deployment.
	Users were prompted with the message `was this useful?' and a thumbsup / thumbsdown option denoting positive / negative feedback.\\
	% \textbf{Takeaway: clinicians find \textsc{QueryAgent} useful.}
}
\label{tab:user-feedback}
\end{table}



Table \ref{tab:conversation-statistics} summarises the statistics of the user conversations.
% The number of words generated by the LLM seems to dominate that of the user, despite our generative LLM having `thinking' turned on enabling it to generate tokens outside of the conversation.
% The number of words in the conversation from tool-calls dominates that of the user and agent combined by an OOM.
The number of turns is the same for user and agent messages, since the agent will respond to each user message.
The number of utterances is the number of discrete messages, counting interleaved text and tool-calls by the agent separtely.
For users this will be the same as the number of turns since a user cannot send two consecutive messages.

\begin{table}[ht]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textsc{QueryAgent} & User & Agent & Tool \\
\hline
	dialogues	& 108 & 108    & -   \\
	turns       & 178 & 178    & -   \\
    	messages	& 178 & 837    & 403   \\
	words 		& 2,221 & 16,892 & 315,429 \\
% 	est. tokens	& 3,184 & 24,322 & 450,612\\
\hline
\end{tabular}
\caption{
Summary statistics for QueryAgent's conversations.
`Tool' indicates the result of a tool-use, e.g. a database search.
\textbf{The majority of words in the conversations were tool call contents.}
}
\label{tab:conversation-statistics}
\end{table}

\section{Conclusion}
% What did we take away from these results?
In this study we described QueryAgent (QA), a novel text-to-query system for epidemiological cohort selection in which an agentic LM determines how best to approach the problem itself rather than following a pre-determined pipeline.

We demonstrated that QA can faithfully and interpretably generate health informatics queries, overcoming a limitation of prior work which noted that queries generated in SQL remain a `black box' to users unfamiliar with the language  (\citet{ziletti_generating_2025}).
Whilst in some cases this may be acceptable, the success of QA is evidence that generating intermediate representations % similar to that proposed  \citet{guo_towards_2019}
is a practical approach to generating queries for users who are unlikely to be familiar with SQL such as medical professionals.

% We found that retrieving the correct medical concepts to be the core challenge for QA.
We found empirically that invoking a sub-agent for the information retrieval component of the task improved performance and reduced the context burden in the main conversation, enabling QA to better maintain coherence in long conversations comprising over 30 distinct tasks.


\section{Limitations}
The limited grammar of our \textit{AND/OR} intermediate representation language limits the range of queries which can be performed by QA.
For instance, custom age-ranges cannot be created, rather the model must rely on existing age-band concepts.


\section{Ethics Statement}
We have carefully designed the system to avoid the possibility of patient data being accessed by the agent.
This has been done by limiting it's capabilities to reading the definitions of concepts and creating queries.
The generated queries select patient information, which only clinical staff with the appropriate permissions can view.

Ethically there is a responsibility to treat patient data as confidental and to share it strictly on a `need to know' basis.
Concepts pertaining to a certain record would be considered confidential information, even if the patient name and other identifiers were pseudonymised
\footnote{Under UK law, the information pertaining to a person is considered confidential even if it is anonymised: \\
\tiny
\url{
https://www.digitalregulations.innovation.nhs.uk/regulations-and-guidance-for-developers/all-developers-guidance/proof-of-concept-using-anonymous-or-artificial-health-data/}
}. 
To ensure that no confidential data can be accessed by the agent, we create a selection of database tools which do not provide access to the `Patient' table, or any tables derived from it; the agent is limited to generating a \textit{query} for patient information, which is executed by the user according to their resective level of permissions when they access the results URL, shown in Figure \ref{fig:question-to-query}.


% Population health management is a serious task, and the use of AI warrants proper ethical consideration.
% To address the ethical concerns regarding this work, let us consider the mechanisms by which such a system could cause harm.

% Since the agent cannot access patient data, we deem the risk of leaking PII to be limited to the uploading of patient information into the dialogue by a user and explicitly warn users at the beginning of each chat to not upload any PII.

% The core risk we identify is that the system generates an incorrect output, leading to a clinician to be misled about the status of one or more patients, resulting in harm to those patient(s) due to the clinician taking a different course of action that they otherwise might, a risk which takes the following forms:

% \begin{enumerate}
% 	\item Errors in selecting concepts
% 	\begin{enumerate}
% 		\item due to incorrect disambiguation e.g. of an overloaded terms
% 		\item due to lack of specialist knowledge e.g. multiple competing database concepts for the same utterance   
% 		\item due to under-specification of the user intent
% 		\item due to no appropriate concept existing 
% 		\item due to confabulation
% 		\item due to random chance
% 	\end{enumerate}
% 	\item Errors in formulating queries
% 	\begin{enumerate}
% 		\item due to semantic parsing errors 
% 		\item due to random chance
% 	\end{enumerate}
% \end{enumerate}

% \textbf{Errors in selecting concepts:}
% Recall that clinical concepts refer to one or more medical codes, possibly with accompanying logic, such as a time-window: `COVID in the last six months' is a concept, for instance.

% Errors in selecting concepts due to the presence of multiple retrieved concepts for a given search are a common error-mode of our system. Many of these concept selection sub-tasks require knowledge of the specific context in which a clinician or analyst created the concept in the first place, making it challenging for the LLM to select the correct one.

% Additionally, LLMs have been known to confabulate/hallucinate when confronted with information they don't understand, something we have observed when handling highly-local knowledge internal to the NHS.

% \textbf{Errors in formulating queries:}
% Formulating the correct AND/OR query for a given utterance is of central importance, as it dictates the cohort of patients which will be selected.
% Conditional on the correct concepts being selected and the user's intent being clear, we find that the tested LLMs consistently generate the correct logical queries.
% In cases where the user intent is unclear, semantic parsing issues become more frequent. Due to the nature of LLMs, we cannot rule out these errors entirely; indeed, the fact that the LLM generates queries highly consistently may lull the user into a false sense of security, leading them to unthinkingly accept a result and thereby generating false knowledge. Our training material for users will note this fact and urge them to remain suspicious of the generated output.

% \textbf{Ethics Summary:} we identify incorrect answers might lead to real-world harm. We address this concern by making the entire answer generation process transparent and interpretable, including the database queries which are generated in a simple, interpretable representation, enabling non-programmers to audit them.


\section*{Acknowledgments}
We thank the NHS Nottingham Data Management Team for their expertise and contributions to this project. We thank Anthony Dranfield and Ben Clarke for their assistance with the eHealthScope platform and database, and Carl Davis for organising the collaboration.

\bibliography{references}

\appendix


\begin{figure*}[b]
  \includegraphics[width=\linewidth]{content/pca_09_16_randomly_sampled_20.png}
  \caption{
    An interactive PCA visualisation of user messages.
  }
  \label{fig:sample-user-utterances}
\end{figure*}


% \section{Example Appendix}
% \label{sec:appendix}
% This is an appendix.

\end{document}
